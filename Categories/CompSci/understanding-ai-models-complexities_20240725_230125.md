# "Understanding AI Models' Complexities"

The transcript discusses the complexities and limitations in understanding AI, particularly in the context of how modern AI models like AlexNet and ChatGPT function. Here are the key points regarding what we don't fully understand about AI:

1. **Intelligence and Operations**: Although AI models like ChatGPT can produce intelligent-seeming outputs, the underlying operations are fundamentally simple, consisting mainly of matrix operations performed by layers of compute blocks (transformers for ChatGPT, convolutional blocks for AlexNet). There is a fundamental question of where true "intelligence" lies in these operations, as what appears intelligent is just a series of calculations.

2. **High-Dimensional Spaces**: AI models operate in high-dimensional embedding spaces, where concepts are represented as points. The complexity of these spaces makes it difficult to interpret how models understand and process information. For example, while we can visualize some learned aspects in simpler models like AlexNet, as models like ChatGPT grow in scale and complexity, our ability to intuitively grasp their functioning diminishes.

3. **Concept Learning**: While we can observe some learned concepts from models (e.g., AlexNet learning to recognize faces without being explicitly trained to do so), there are often many other representations and relationships that remain opaque. The sheer scale of what AI learns from data means there could be numerous concepts that models learn that we cannot easily identify or articulate.

4. **Scaling Issues**: The advent of deep learning has shown that scaling data and computational power can lead to breakthroughs in performance. However, predicting future advancements is notoriously difficult. The transcript suggests that just scaling up might not be sufficient; new approaches might emerge or older methods may resurface unexpectedly.

5. **Explainability**: As AI models become more sophisticated, understanding their inner workings becomes increasingly challenging. While techniques like activation atlases give us some insights into what models are learning, they often only reveal a fraction of the information that is encoded in the model's parameters, making it hard for researchers to explain how and why a model makes specific decisions.

6. **Semantic Meaning**: The directions in embedding spaces can hold semantic meaning, but understanding these meanings and their implications for model behavior is still an area of ongoing research. For instance, how specific activations can correspond to complex language concepts illustrates a level of functionality that we don't fully grasp.

In summary, while we have made significant strides in understanding how AI works, especially with advances like deep learning, there remain fundamental challenges in grasping the intricacies of intelligence, the interpretability of high-dimensional representations, the nature of learned concepts, and predicting future developments in AI technology.